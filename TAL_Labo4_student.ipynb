{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/HEIG-VD_Logo_96x29_RVB_ROUGE.png\" alt=\"HEIG-VD Logo\" width=\"250\"/>\n",
    "\n",
    "# Cours TAL - Laboratoire 4\n",
    "# Reconnaissance d'entités nommées (NER)\n",
    "\n",
    "**Objectifs**\n",
    "\n",
    "Appliquer l'outil de *Named Entity Recognition* fourni par NLTK sur le corpus Reuters en anglais, puis évaluer sa performance sur les données de test CoNLL 2003 possédant une annotation de référence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Expériences avec la NER de NLTK\n",
    "\n",
    "Le **but de cette partie** est d'utiliser le reconnaisseur d'entités nommées de NLTK pour extraire les entités nommées les plus fréquentes du corpus Reuters, avec leur type.\n",
    "* le NER de NLTK est tout simplement la fonction `nltk.ne_chunk`, qui s'applique sur un texte tokenisé, avec les POS tags -- la fonction est documentée dans le [livre NLTK, ch.7](http://www.nltk.org/book/ch07.html), section 5 (tout à la fin) ;\n",
    "*  le corpus Reuters contient environ 10'000 dépêches datant des années 1980, et il est fourni avec NLTK comme expliqué dans le [livre NLTK, ch.2](http://www.nltk.org/book/ch02.html), §1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/kler/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "nltk.downloader.Downloader().download('reuters') \n",
    "# à exécuter une seule fois pour télécharger les fichiers localement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En suivant les exemples fournis dans le livre NLTK, veuillez écrire le code qui permet de répondre aux questions suivantes, et écrire vos réponses dans une cellule *text markdown* ensuite.\n",
    "* Combien de fichiers (`fileids`) le corpus Reuters contient-il ?\n",
    "* Combien de phrases le corpus contient-il ?  (note : un seul appel de fonction est nécessaire)\n",
    "* Combien de mots le corpus contient-il ?  (note : un seul appel de fonction est nécessaire)\n",
    "* Veuillez afficher 5 *fileids* de votre choix (les noms, pas les contenus)\n",
    "* Pour un fichier (*fileid*) de votre choix, veuillez afficher son texte brut, puis la liste de ses phrases, puis enfin la liste de ses mots (avec la tokenization de référence du corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of file : 10788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-47-078e9cecd540>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'number of file :'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mNumberFiles\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mNumberSentences\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mreuters\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'number of sentence :'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mNumberSentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mNumberWords\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mreuters\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'number of word :'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mNumberWords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/util.py\u001B[0m in \u001B[0;36m__len__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    387\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_offsets\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pieces\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    388\u001B[0m             \u001B[0;31m# Iterate to the end of the corpus.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 389\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mtok\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miterate_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_offsets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    390\u001B[0m                 \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/util.py\u001B[0m in \u001B[0;36miterate_from\u001B[0;34m(self, start_tok)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    411\u001B[0m             \u001B[0;31m# Get everything we can from this piece.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 412\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mtok\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpiece\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miterate_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_tok\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0moffset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    413\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0mtok\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    414\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/util.py\u001B[0m in \u001B[0;36miterate_from\u001B[0;34m(self, start_tok)\u001B[0m\n\u001B[1;32m    280\u001B[0m         \u001B[0;31m# Open the stream, if it's not open already.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stream\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 282\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    283\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0;31m# If the file is empty, the while loop will never run.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/util.py\u001B[0m in \u001B[0;36m_open\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \"\"\"\n\u001B[1;32m    204\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fileid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPathPointer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stream\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fileid\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_encoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_encoding\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             self._stream = SeekableUnicodeStreamReader(\n",
      "\u001B[0;32m~/.local/lib/python3.7/site-packages/nltk/data.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(self, encoding)\u001B[0m\n\u001B[1;32m    438\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    439\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 440\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_zipfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_entry\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    441\u001B[0m         \u001B[0mstream\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBytesIO\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    442\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_entry\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mendswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".gz\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.7/site-packages/nltk/data.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    944\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    945\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 946\u001B[0;31m         \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mzipfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mZipFile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    947\u001B[0m         \u001B[0;31m# Ensure that _fileRefCnt needs to be set for Python2and3 compatible code.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    948\u001B[0m         \u001B[0;31m# Since we only opened one file here, we add 1.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/CoursTAL/lib/python3.7/zipfile.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, name, pwd)\u001B[0m\n\u001B[1;32m   1463\u001B[0m         \u001B[0;34m\"\"\"Return file bytes for name.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1464\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpwd\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mfp\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1465\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1466\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1467\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpwd\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforce_zip64\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/CoursTAL/lib/python3.7/zipfile.py\u001B[0m in \u001B[0;36mclose\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1040\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1041\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_close_fileobj\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1042\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fileobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1043\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1044\u001B[0m             \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/CoursTAL/lib/python3.7/zipfile.py\u001B[0m in \u001B[0;36mclose\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    760\u001B[0m             \u001B[0mfileobj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    761\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_file\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_close\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfileobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m \u001B[0;31m# Provide the tell method for unseekable stream\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/CoursTAL/lib/python3.7/zipfile.py\u001B[0m in \u001B[0;36m_fpclose\u001B[0;34m(self, fp)\u001B[0m\n\u001B[1;32m   1937\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fileRefCnt\u001B[0m \u001B[0;34m-=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1938\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fileRefCnt\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_filePassed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1939\u001B[0;31m             \u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1940\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1941\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "NumberFiles = reuters.fileids()\n",
    "print('number of file :', len(NumberFiles))\n",
    "NumberSentences= reuters.sents()\n",
    "print('number of sentence :', len(NumberSentences))\n",
    "NumberWords= reuters.words()\n",
    "print('number of word :', len(NumberWords))\n",
    "FiveFields = reuters.fileids()[:5]\n",
    "print('Five choice Fields :', FiveFields)\n",
    "ContentField = reuters.raw(fileids = ['test/14826'])\n",
    "print ('raw (test/14826) :', ContentField)\n",
    "ListSentenceField= reuters.sents('test/14826')\n",
    "print('List of sentence of fields test/14826 :', ListSentenceField)\n",
    "ListWordField= reuters.words('test/14826')\n",
    "print('List of Word of fields test/14826 :', ListWordField)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vous demande maintenant d'expérimenter avec un seul texte du corpus Reuters, pour en extraire les entités nommées.   Veuillez répondre aux questions suivantes.\n",
    "\n",
    "* À partir du texte brut (*raw*) d'une dépêche de votre choix, effectuez la segmentation en phrases, et affichez le nombre de phrases\n",
    "* Sur une phrase de votre choix, effectuez avec NLTK la tokenization, le POS tagging et la NER, et affichez le résultat. (Si la phrase ne contient pas d'entité nommée (*chunk*), veuillez en choisir une autre.)\n",
    "* Quel est le type d'objet que retourne `ne_chunk` ?\n",
    "* Quelle est l'effet de l'attribut `binary` (True ou False) dans l'appel de `nltk.ne_chunk` ?\n",
    "* Veuillez rassembler les entités nommées de la phrase en une seule liste, de la forme `[('MTBE', 'ORGANIZATION'), ('United States', 'GPE')]` (veuillez notamment joindre en une seule chaîne les mots des entités à plusieurs mots).\n",
    "\n",
    "Pour votre information, le `ne_chunk()` de NLTK annote les types suivants d'entités nommées : ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, PERCENT, FACILITY, GPE (= *geo-political entity*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "#nltk.download('words')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "\n",
    "ContentField = reuters.raw(fileids = ['test/14826'])\n",
    "sentences = nltk.sent_tokenize(ContentField)\n",
    "print('number of sentence :', len(sentences))\n",
    "chunked = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentences[0])))\n",
    "print(chunked)\n",
    "print('# type objet que retourne ne_chunk :', type(chunked))\n",
    "# Avec le paramètre binary = True, les entités nommées sont simplement marquées comme NE; \n",
    "# sinon, le classificateur ajoute des étiquettes de catégorie telles que PERSONNE, ORGANISATION et GPE. \n",
    "\n",
    "places =[]\n",
    "for ne in chunked:\n",
    "        if len(ne) == 1:\n",
    "            if (ne.label() == 'GPE' or ne.label() == 'ORGANIZATION'):\n",
    "                    places.append((ne[0][0], ne.label()))\n",
    "\n",
    "print(places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Veuillez écrire une fonction, commençant par la ligne `def extract_named_entities(text):` qui retourne la liste des entités nommées avec leur type, à partir d'un texte donné (string).  Inspirez-vous de votre réponse à la dernière question ci-dessus.\n",
    "* Testez votre fonction sur le texte que vous avez utilisé ci-dessus.  \n",
    "* Observez vous des erreurs de détection (rappel ou précision) et/ou d'étiquetage ?  Merci d'en indiquer quelques-unes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "def extract_named_entities(text): \n",
    "    places = []\n",
    "    text = nltk.word_tokenize(text)\n",
    "    nes = nltk.ne_chunk(nltk.pos_tag(text))\n",
    "    for ne in nes:\n",
    "        if len(ne) == 1:\n",
    "            if (ne.label() == 'GPE' or ne.label() == 'PERSON' or ne.label() == 'LOCATION' or ne.label() == 'DATE' or \n",
    "                ne.label() == 'TIME' or ne.label() == 'MONEY' or ne.label() == 'PERCENT' or ne.label() == 'FACILITY' or ne.label() == 'ORGANIZATION'):\n",
    "                places.append((ne[0][0], ne.label()))\n",
    "    print(places)\n",
    "\n",
    "extract_named_entities(\"In Hong Kong, where newspapers have alleged Japan has been selling elow-cost semiconductors, some electronics manufacturers share that view. But other businessmen said such a short-term commercial advantage would be outweighed by further U.S. Pressure to block imports.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "ContentField = reuters.raw(fileids = ['test/14826'])\n",
    "sentences = nltk.sent_tokenize(ContentField)\n",
    "extract_named_entities(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le commentaire sur les erreurs observées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez parcourir tous les textes du corpus Reuters et collecter toutes les entités nommées dans une liste.  Créez ensuite une `FreqDist` et affichez les 30 NE les plus fréquentes avec leur nombre d'occurrences.  Combien de temps approximativement prend cette opération ?  (Suggestion : augmentez progressivement le nombre de fileids que vous traitez, pour estimer le temps total.)  Veuillez commenter le résultat obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "\n",
    "sentences = reuters.words()\n",
    "sentences = nltk.pos_tag(sentences) \n",
    "sentences = nltk.ne_chunk(sentences, binary=True)\n",
    "places =[]\n",
    "for ne in sentences:\n",
    "        if len(ne) == 1:\n",
    "            if (ne.label() == 'NE'):\n",
    "                places.append((ne[0][0], ne.label()))\n",
    "\n",
    "print(places)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici les commentaires sur le résultat.\n",
    "varNE = nltk.FreqDist(places)\n",
    "\n",
    "print('affichez les 30 NE les plus fréquentes avec leur nombre doccurrences: ',varNE.most_common(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quel est le nombre total de NE trouvées (occurrences pas nécessairement différentes) et quel est le nombre de NE différentes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire pour répondre aux deux questions.\n",
    "print('Nombre total de NE Trouver: ', varNE.N())\n",
    "print('Nombre de NE différentes: ', len(varNE.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Évaluer la fonction de NER de NLTK sur les données CoNLL 2003\n",
    "\n",
    "À la conférence [CoNLL](https://www.clips.uantwerpen.be/pages/past-workshops) 2003, une des tâches compétitives consistait à tester des systèmes de NER sur l'anglais (voir [la description de la tâche et les scores obtenus](https://www.clips.uantwerpen.be/conll2003/ner/)).  Les ressources annotées ne sont pas disponibles via CoNLL, mais on peut en trouver [une copie sur le web](https://sourceforge.net/p/text-analysis/svn/1243/tree/text-analysis/trunk/Corpora/CoNLL/2003/) (une [autre copie](https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003) est aussi disponible).  Les textes proviennent du [corpus Reuters](http://trec.nist.gov/data/reuters/reuters.html).  Pour mémoire, une autre source de données est le [corpus WikiNER](https://github.com/dice-group/FOX/tree/master/input/Wikiner).\n",
    "\n",
    "Le format d'annotation comprend 4 colonnes séparées par un espace.  Ce format ressemble au format \"conll\" que nous avons utilisé pour le *POS tagging* et le *parsing*.  Chaque ligne correspond à un mot, et une ligne vide sépare les phrases.  Sur chaque ligne, le 1er item est le mot, le 2e est le POS tag, le 3e est un tag qui indique le groupe syntaxique, et le 4e le tag qui indique l'entité nommée.  (À vous d'étudier ce tag plus en détail.)  Il y a des données d'entraînement (`eng.train`), et trois fichiers de test (`eng.testa`, `eng.testb` et `eng.testc`, le 2e ayant servi pour l'évaluation finale).\n",
    "\n",
    "**Travail demandé**\n",
    "\n",
    "Les questions qui suivent (inspirées des étapes de ce [tutoriel en ligne](https://pythonprogramming.net/testing-stanford-ner-taggers-for-accuracy/)) vous permettront d'estimer la \"justesse\" du NER de NLTK  sur les données CoNLL (seulement l'*accuracy*, pas le rappel et la précision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le premier objectif** est d'importer les données CoNLL 2003 dans ce notebook et adapter leur format pour qu'il soit comparable à celui de `nltk.ne_chunk()`, ce qui nécessite aussi la modification de l'output de cette fonction.\n",
    "\n",
    "En examinant les fichiers `eng.testa`, `eng.testb` et `eng.testc`, décrivez brièvement le format d'annotation CoNLL (2-3 phrases) et indiquez les types de NER annotés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici votre réponse.\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "eng_test_a = \"eng.testa\"\n",
    "eng_test_b = \"eng.testb\"\n",
    "eng_test_c = \"eng.testc\"\n",
    "\n",
    "files = [eng_test_a, eng_test_b, eng_test_c]\n",
    "\n",
    "for file in files :\n",
    "    if os.path.exists(file):\n",
    "        fd = open(file, \"r\", encoding='utf-8')\n",
    "        content = fd.read()\n",
    "        #tokens = nltk.word_tokenize(content)\n",
    "        #print('Tokens in', file, \" : \", content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez écrire une fonction qui ouvre un fichier CoNLL (p.ex. `eng.testa`) et crée deux listes :\n",
    "1. la liste des paires (token, pos_tag) que vous passerez plus tard à `ne_chunk()`;\n",
    "2. la liste des paires (token, ner_tag) où ner_tag est l'une des catégories de NER que vous avez indiquées plus haut.\n",
    "Ces listes seront stockées comme ci-dessous.  Appelez ensuite cette fonction sur les trois fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voici comment stocker les données dans une structure:\n",
    "filenames = ['eng.testa', 'eng.testb', 'eng.testc']\n",
    "test_data = dict()\n",
    "for f in filenames:\n",
    "    test_data[f] = dict()\n",
    "    test_data[f]['words'] = []\n",
    "    test_data[f]['keytags'] = [] # 'key' signifie correct\n",
    "    test_data[f]['reptags'] = [] # 'rep' pour réponse du système    \n",
    "# def conll2nltk(filename='eng.testa'):\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code de la fonction.\n",
    "# Group NE data into tuples\n",
    "def group(lst, n):\n",
    "  for i in range(0, len(lst), n):\n",
    "    val = lst[i:i+n]\n",
    "    if len(val) == n:\n",
    "      yield tuple(val)\n",
    "\n",
    "def conll2nltk(filename='eng.testa'):\n",
    "    raw_annotations = open(filename).read()\n",
    "    split_annotations = raw_annotations.split()\n",
    "\n",
    "    # Amend class annotations to reflect Stanford's NERTagger\n",
    "    for (n,i) in enumerate(split_annotations):\n",
    "        if i == \"I-PER\":\n",
    "            split_annotations[n] = \"PERSON\"\n",
    "        if i == \"I-ORG\":\n",
    "            split_annotations[n] = \"ORGANIZATION\"\n",
    "        if i == \"I-LOC\":\n",
    "            split_annotations[n] = \"LOCATION\"\n",
    "\n",
    "    reference_annotations = list(group(split_annotations, 4))\n",
    "\n",
    "    w_pos = []\n",
    "    w_ner = []\n",
    "    for (w, pos, syn, ner ) in reference_annotations:\n",
    "        w_pos.append((w,pos))\n",
    "        w_ner.append((w,ner))\n",
    "\n",
    "    return w_pos, w_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('West', 'NNP'), ('Indian', 'NNP'), ('all-rounder', 'NN'), ('Phil', 'NNP'), ('Simmons', 'NNP'), ('took', 'VBD'), ('four', 'CD'), ('for', 'IN'), ('38', 'CD'), ('on', 'IN'), ('Friday', 'NNP'), ('as', 'IN'), ('Leicestershire', 'NNP'), ('beat', 'VBD'), ('Somerset', 'NNP'), ('by', 'IN'), ('an', 'DT'), ('innings', 'NN'), ('and', 'CC'), ('39', 'CD'), ('runs', 'NNS'), ('in', 'IN'), ('two', 'CD'), ('days', 'NNS'), ('to', 'TO'), ('take', 'VB'), ('over', 'IN'), ('at', 'IN'), ('the', 'DT'), ('head', 'NN'), ('of', 'IN'), ('the', 'DT'), ('county', 'NN'), ('championship', 'NN'), ('.', '.'), ('Their', 'PRP$'), ('stay', 'NN'), ('on', 'IN'), ('top', 'NN'), (',', ','), ('though', 'RB'), (',', ','), ('may', 'MD'), ('be', 'VB'), ('short-lived', 'JJ'), ('as', 'IN'), ('title', 'NN'), ('rivals', 'NNS'), ('Essex', 'NNP'), (',', ','), ('Derbyshire', 'NNP'), ('and', 'CC'), ('Surrey', 'NNP'), ('all', 'DT'), ('closed', 'VBD'), ('in', 'RP'), ('on', 'IN'), ('victory', 'NN'), ('while', 'IN'), ('Kent', 'NNP'), ('made', 'VBD'), ('up', 'RP'), ('for', 'IN'), ('lost', 'VBN'), ('time', 'NN'), ('in', 'IN'), ('their', 'PRP$'), ('rain-affected', 'JJ'), ('match', 'NN'), ('against', 'IN'), ('Nottinghamshire', 'NNP'), ('.', '.')]\n",
      "[('West', 'I-MISC'), ('Indian', 'I-MISC'), ('all-rounder', 'O'), ('Phil', 'PERSON'), ('Simmons', 'PERSON'), ('took', 'O'), ('four', 'O'), ('for', 'O'), ('38', 'O'), ('on', 'O'), ('Friday', 'O'), ('as', 'O'), ('Leicestershire', 'ORGANIZATION'), ('beat', 'O'), ('Somerset', 'ORGANIZATION'), ('by', 'O'), ('an', 'O'), ('innings', 'O'), ('and', 'O'), ('39', 'O'), ('runs', 'O'), ('in', 'O'), ('two', 'O'), ('days', 'O'), ('to', 'O'), ('take', 'O'), ('over', 'O'), ('at', 'O'), ('the', 'O'), ('head', 'O'), ('of', 'O'), ('the', 'O'), ('county', 'O'), ('championship', 'O'), ('.', 'O'), ('Their', 'O'), ('stay', 'O'), ('on', 'O'), ('top', 'O'), (',', 'O'), ('though', 'O'), (',', 'O'), ('may', 'O'), ('be', 'O'), ('short-lived', 'O'), ('as', 'O'), ('title', 'O'), ('rivals', 'O'), ('Essex', 'ORGANIZATION'), (',', 'O'), ('Derbyshire', 'ORGANIZATION'), ('and', 'O'), ('Surrey', 'ORGANIZATION'), ('all', 'O'), ('closed', 'O'), ('in', 'O'), ('on', 'O'), ('victory', 'O'), ('while', 'O'), ('Kent', 'ORGANIZATION'), ('made', 'O'), ('up', 'O'), ('for', 'O'), ('lost', 'O'), ('time', 'O'), ('in', 'O'), ('their', 'O'), ('rain-affected', 'O'), ('match', 'O'), ('against', 'O'), ('Nottinghamshire', 'ORGANIZATION'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez appliquer la fonction aux trois noms de fichier fournis.\n",
    "test_data2 = dict()\n",
    "for f in filenames:\n",
    "    test_data2[f] = dict()\n",
    "    test_data2[f]['token_pos'] = []\n",
    "    test_data2[f]['token_ner'] = []\n",
    "\n",
    "    test = conll2nltk(f)\n",
    "    test_data2[f]['token_pos']= test[0]\n",
    "    test_data2[f]['token_ner'] = test[1]\n",
    "\n",
    "    #reference annotation\n",
    "    test_data[f]['keytags'] = test[1]\n",
    "\n",
    "print(test_data2['eng.testc']['token_pos'])\n",
    "print(test_data2['eng.testc']['token_ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le second objectif** est de donner les mots à `ne_chunk()`, obtenir le résultat de la NER, et le stocker dans la même forme que l'annotation de référence (des paires de (token, TAG) pour tous les tokens).  Ces résultats seront ajoutés à la structure `test_data` dans le champ 'reptags' (pour \"response tags\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code pour les 3 datasets.\n",
    "def getNltkNE(filename='eng.testa'):\n",
    "    raw_annotations = open(filename).read()\n",
    "    split_annotations = raw_annotations.split()\n",
    "    pure_tokens = split_annotations[::4]\n",
    "    #print(pure_tokens)\n",
    "\n",
    "    tagged_words = nltk.pos_tag(pure_tokens)\n",
    "    nltk_unformatted_prediction = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "    #Convert prediction to multiline string and then to list (includes pos tags)\n",
    "    multiline_string = nltk.chunk.tree2conllstr(nltk_unformatted_prediction)\n",
    "    listed_pos_and_ne = multiline_string.split()\n",
    "\n",
    "    # Delete pos tags and rename\n",
    "    del listed_pos_and_ne[1::3]\n",
    "    listed_ne = listed_pos_and_ne\n",
    "\n",
    "    # Group prediction into tuples\n",
    "    nltk_formatted_prediction = list(group(listed_ne, 2))\n",
    "    #print(nltk_formatted_prediction)\n",
    "    return nltk_formatted_prediction\n",
    "\n",
    "\n",
    "for f in filenames:\n",
    "    test = conll2nltk(f)\n",
    "    test_data[f]['reptags']= getNltkNE(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez vérifier, pour chaque fichier, que les listes 'keytags' et 'reptags' ont le même nombre de mots, en les affichant côte à côte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng.testa  keytags :  51578 \treptags :  51578\n",
      "eng.testb  keytags :  46666 \treptags :  46666\n",
      "eng.testc  keytags :  72 \treptags :  72\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire ici le code pour les 3 datasets.\n",
    "for f in filenames:\n",
    "    print(f, ' keytags : ',len(test_data[f]['keytags']), '\\treptags : ',len(test_data[f]['reptags']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le 3e et dernier objectif** est de calculer la pourcentage d'étiquettes correctes dans les trois fichiers par rapport au nombre de mots de chacun.  \n",
    "\n",
    "Pour ce faire, remarquez que les étiquettes assignées par NLTK sont ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, PERCENT, FACILITY, GPE (= geo-political entity), alors que celles des fichiers CoNLL sont celles que vous avez indiquées en réponse plus haut.  Pour comptabiliser les réponses correctes, il faut d'abord définir une fonction appelée `compatible(n, c)` qui indique si deux étiquettes (l'une de NLTK, l'autre de CoNLL) sont conceptuellement identiques (c'est-à-dire qu'elles ont la même signification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng.testa  keytags :  ('victory', 'O')\n",
      "reptags :  ('victory', 'O')\n",
      "compat :  True\n",
      "eng.testb  keytags :  ('China', 'LOCATION')\n",
      "reptags :  ('China', 'B-GPE')\n",
      "compat :  True\n",
      "eng.testc  keytags :  ('.', 'O')\n",
      "reptags :  ('.', 'O')\n",
      "compat :  True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Veuillez définir ici une fonction de comparaison des tags NER.\n",
    "def compatible(t1, t2):\n",
    "    def compOneWay(tag1, tag2):\n",
    "        comp = tag1\n",
    "        if tag1 == \"B-PERSON\" or tag1 == \"I-PERSON\":\n",
    "            comp = \"PERSON\"\n",
    "        if tag1 == \"B-ORGANIZATION\" or tag1 == \"I-ORGANIZATION\":\n",
    "            comp = \"ORGANIZATION\"\n",
    "        if tag1 == \"B-LOCATION\" or tag1 == \"I-LOCATION\" or tag1== \"B-GPE\" or tag1 == \"I-GPE\":\n",
    "            comp = \"LOCATION\"\n",
    "        return comp == tag2\n",
    "\n",
    "    return compOneWay(t1, t2) or compOneWay(t2, t1)\n",
    "\n",
    "for f in filenames:\n",
    "    print(f, ' keytags : ',test_data[f]['keytags'][71])\n",
    "    print('reptags : ',test_data[f]['reptags'][71])\n",
    "    print('compat : ', compatible(test_data[f]['keytags'][71][1],test_data[f]['reptags'][71][1]))\n",
    "\n",
    "print(compatible('C-PERSON', 'PERSON'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez définir ici une fonction qui compare deux listes de (mot, tag) et qui\n",
    "# retourne le pourcentage de (mot, tag) identiques selon la fonction \"compatible()\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez appliquer ici la fonction ci-dessus et afficher les scores sur les 3 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note sur `nltk.ne_chunk()`.** Pour mémoire, signalons que le résultat de `nltk.ne_chunk()`, qui est un arbre, peut être transformé en une chaîne de caractères multi-lignes formatée selon les guidelines CoNLL grâce à la méthode `nltk.chunk.tree2conllstr()`.  Par ailleurs, pour comparer deux étiquetages de mots (listes de paires (mot, tag)), on peut utiliser directement la fonction `nltk.metrics.scores.accuracy` de NLTK, pour autant que les tags soient comparables avec `==`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarque finale\n",
    "Il est possible d'appliquer de la même façon [l'outil de NER avec CRF fourni par Stanford](https://nlp.stanford.edu/software/CRF-NER.html).  Les CRF, *Conditional Random Fields*, sont le modèle probabiliste qui est utilisé dans cet outil.\n",
    "\n",
    "Comme dans le cas du *POS tagger* CoreNLP de Stanford, on peut invoquer l'outil en ligne de commande (suivant les exemples fournis [ici](https://nlp.stanford.edu/software/CRF-NER.html#Starting) ou [ici](https://nlp.stanford.edu/software/crf-faq.shtml), notamment pour les [options de sortie](https://nlp.stanford.edu/software/crf-faq.shtml#j)).  Ou alors, on peut utiliser le [wrapper `StanfordNERTagger` de NLTK](http://www.nltk.org/api/nltk.tag.html?#nltk.tag.stanford.StanfordNERTagger).  On peut alors voir que les performances sont plus élevées que celles de `nltk.ne_chunk()`. \n",
    "\n",
    "Une [version plus élaborée de NER est fournie par Stanford dans le cadre de la boîte à outils CoreNLP](https://stanfordnlp.github.io/CoreNLP/ner.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du laboratoire 4\n",
    "\n",
    "Merci de nettoyer votre feuille, exécuter une dernière fois toutes les instructions, sauvegarder le résultat, et le soumettre sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}